{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  166466\n",
      "Total Vocab:  91\n"
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os\n",
    "import sys\n",
    "wordlists = PlaintextCorpusReader(\"Nepali_Corpus\", '.*txt')\n",
    "data = wordlists.fileids()[:100]\n",
    "text = []\n",
    "for i in data:\n",
    "    with open(os.path.join(\"Nepali_Corpus\",i)) as file:\n",
    "        text.append(file.read())\n",
    "text = \"\\n\".join(text)\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = text[i:i + seq_length]\n",
    "    seq_out = text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  166366\n"
     ]
    }
   ],
   "source": [
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59340659],\n",
       "       [0.32967033],\n",
       "       [0.63736264],\n",
       "       [0.69230769],\n",
       "       [0.53846154],\n",
       "       [0.54945055],\n",
       "       [0.72527473],\n",
       "       [0.61538462],\n",
       "       [0.01098901],\n",
       "       [0.94505495],\n",
       "       [0.01098901],\n",
       "       [0.56043956],\n",
       "       [0.7032967 ],\n",
       "       [0.62637363],\n",
       "       [0.8021978 ],\n",
       "       [0.59340659],\n",
       "       [0.01098901],\n",
       "       [0.95604396],\n",
       "       [0.68131868],\n",
       "       [0.72527473],\n",
       "       [0.61538462],\n",
       "       [0.8021978 ],\n",
       "       [0.61538462],\n",
       "       [0.75824176],\n",
       "       [0.96703297],\n",
       "       [0.62637363],\n",
       "       [0.75824176],\n",
       "       [0.01098901],\n",
       "       [0.61538462],\n",
       "       [0.7032967 ],\n",
       "       [0.62637363],\n",
       "       [0.7032967 ],\n",
       "       [0.40659341],\n",
       "       [0.01098901],\n",
       "       [0.59340659],\n",
       "       [0.7032967 ],\n",
       "       [0.49450549],\n",
       "       [0.7032967 ],\n",
       "       [0.01098901],\n",
       "       [0.53846154],\n",
       "       [0.40659341],\n",
       "       [0.7032967 ],\n",
       "       [0.32967033],\n",
       "       [0.7032967 ],\n",
       "       [0.28571429],\n",
       "       [0.67032967],\n",
       "       [0.17582418],\n",
       "       [0.35164835],\n",
       "       [0.76923077],\n",
       "       [0.01098901],\n",
       "       [0.54945055],\n",
       "       [0.8021978 ],\n",
       "       [0.61538462],\n",
       "       [0.38461538],\n",
       "       [0.69230769],\n",
       "       [0.61538462],\n",
       "       [0.54945055],\n",
       "       [0.8021978 ],\n",
       "       [0.61538462],\n",
       "       [0.67032967],\n",
       "       [0.69230769],\n",
       "       [0.61538462],\n",
       "       [0.59340659],\n",
       "       [0.69230769],\n",
       "       [0.01098901],\n",
       "       [0.49450549],\n",
       "       [0.71428571],\n",
       "       [0.63736264],\n",
       "       [0.8021978 ],\n",
       "       [0.61538462],\n",
       "       [0.49450549],\n",
       "       [0.69230769],\n",
       "       [0.01098901],\n",
       "       [0.51648352],\n",
       "       [0.7032967 ],\n",
       "       [0.28571429],\n",
       "       [0.32967033],\n",
       "       [0.78021978],\n",
       "       [0.01098901],\n",
       "       [0.3956044 ],\n",
       "       [0.01098901],\n",
       "       [0.81318681],\n",
       "       [0.01098901],\n",
       "       [0.64835165],\n",
       "       [0.53846154],\n",
       "       [0.7032967 ],\n",
       "       [0.57142857],\n",
       "       [0.69230769],\n",
       "       [0.61538462],\n",
       "       [0.01098901],\n",
       "       [0.68131868],\n",
       "       [0.75824176],\n",
       "       [0.43956044],\n",
       "       [0.79120879],\n",
       "       [0.18681319],\n",
       "       [0.46153846],\n",
       "       [0.69230769],\n",
       "       [0.01098901],\n",
       "       [0.21978022],\n",
       "       [0.23076923]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]/n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166366, 100, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "166366/166366 [==============================] - 730s 4ms/step - loss: 3.4115\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.41154, saving model to weights-improvement-01-3.4115.hdf5\n",
      "Epoch 2/20\n",
      "166366/166366 [==============================] - 692s 4ms/step - loss: 3.2199\n",
      "\n",
      "Epoch 00002: loss improved from 3.41154 to 3.21993, saving model to weights-improvement-02-3.2199.hdf5\n",
      "Epoch 3/20\n",
      "166366/166366 [==============================] - 688s 4ms/step - loss: 3.1118\n",
      "\n",
      "Epoch 00003: loss improved from 3.21993 to 3.11184, saving model to weights-improvement-03-3.1118.hdf5\n",
      "Epoch 4/20\n",
      "166366/166366 [==============================] - 733s 4ms/step - loss: 3.0102\n",
      "\n",
      "Epoch 00004: loss improved from 3.11184 to 3.01019, saving model to weights-improvement-04-3.0102.hdf5\n",
      "Epoch 5/20\n",
      "166366/166366 [==============================] - 681s 4ms/step - loss: 2.9220\n",
      "\n",
      "Epoch 00005: loss improved from 3.01019 to 2.92203, saving model to weights-improvement-05-2.9220.hdf5\n",
      "Epoch 6/20\n",
      "166366/166366 [==============================] - 666s 4ms/step - loss: 2.8526\n",
      "\n",
      "Epoch 00006: loss improved from 2.92203 to 2.85258, saving model to weights-improvement-06-2.8526.hdf5\n",
      "Epoch 7/20\n",
      "166366/166366 [==============================] - 667s 4ms/step - loss: 2.7946\n",
      "\n",
      "Epoch 00007: loss improved from 2.85258 to 2.79459, saving model to weights-improvement-07-2.7946.hdf5\n",
      "Epoch 8/20\n",
      "166366/166366 [==============================] - 668s 4ms/step - loss: 2.7437\n",
      "\n",
      "Epoch 00008: loss improved from 2.79459 to 2.74367, saving model to weights-improvement-08-2.7437.hdf5\n",
      "Epoch 9/20\n",
      "166366/166366 [==============================] - 855s 5ms/step - loss: 2.6989\n",
      "\n",
      "Epoch 00009: loss improved from 2.74367 to 2.69893, saving model to weights-improvement-09-2.6989.hdf5\n",
      "Epoch 10/20\n",
      "166366/166366 [==============================] - 895s 5ms/step - loss: 2.6561\n",
      "\n",
      "Epoch 00010: loss improved from 2.69893 to 2.65606, saving model to weights-improvement-10-2.6561.hdf5\n",
      "Epoch 11/20\n",
      "166366/166366 [==============================] - 836s 5ms/step - loss: 2.6173\n",
      "\n",
      "Epoch 00011: loss improved from 2.65606 to 2.61726, saving model to weights-improvement-11-2.6173.hdf5\n",
      "Epoch 12/20\n",
      "166366/166366 [==============================] - 860s 5ms/step - loss: 2.5800\n",
      "\n",
      "Epoch 00012: loss improved from 2.61726 to 2.57999, saving model to weights-improvement-12-2.5800.hdf5\n",
      "Epoch 13/20\n",
      "166366/166366 [==============================] - 843s 5ms/step - loss: 2.5462\n",
      "\n",
      "Epoch 00013: loss improved from 2.57999 to 2.54617, saving model to weights-improvement-13-2.5462.hdf5\n",
      "Epoch 14/20\n",
      "166366/166366 [==============================] - 795s 5ms/step - loss: 2.5137\n",
      "\n",
      "Epoch 00014: loss improved from 2.54617 to 2.51375, saving model to weights-improvement-14-2.5137.hdf5\n",
      "Epoch 15/20\n",
      "166366/166366 [==============================] - 878s 5ms/step - loss: 2.4827\n",
      "\n",
      "Epoch 00015: loss improved from 2.51375 to 2.48271, saving model to weights-improvement-15-2.4827.hdf5\n",
      "Epoch 16/20\n",
      "166366/166366 [==============================] - 907s 5ms/step - loss: 2.4534\n",
      "\n",
      "Epoch 00016: loss improved from 2.48271 to 2.45335, saving model to weights-improvement-16-2.4534.hdf5\n",
      "Epoch 17/20\n",
      " 85888/166366 [==============>...............] - ETA: 11:08 - loss: 2.4204"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(LSTM(256))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# load the network weights\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "gen_data = [int_to_char[value] for value in pattern]\n",
    "gen_data.append(\"***\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    print(prediction)\n",
    "    result = int_to_char[index]\n",
    "    try:\n",
    "        gen_data.append(int_to_char[result])\n",
    "    except:\n",
    "        gen_data.append(\" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join(gen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
