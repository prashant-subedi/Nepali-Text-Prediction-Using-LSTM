{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "import sys\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os\n",
    "import sys\n",
    "wordlists = PlaintextCorpusReader(\"Nepali_Corpus\", '.*txt')\n",
    "data = wordlists.fileids()[:10]\n",
    "text = []\n",
    "invalid = 0\n",
    "for i in data:\n",
    "    with open(os.path.join(\"Nepali_Corpus\",i),encoding='utf8') as file:\n",
    "        try:\n",
    "            text.append(file.read())\n",
    "        except UnicodeDecodeError :\n",
    "            print(i)\n",
    "            invalid+=1\n",
    "print(invalid)\n",
    "raw_text = \"\".join(text)\n",
    "raw_text = raw_text.replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '(', ')', ',', '.', '?', '\\xa0', '¥', 'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ए', 'ऐ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'े', 'ै', 'ो', 'ौ', '्', '।', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '–', '‘', '’']\n",
      "Total Characters:  13424\n",
      "Total Vocab:  78\n",
      "Total Patterns:  13224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 200\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13224/13224 [==============================] - 397s 30ms/step - loss: 3.5163\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.51626, saving model to weights-improvement-01-3.5163-bigger.hdf5\n",
      "Epoch 2/50\n",
      "13224/13224 [==============================] - 420s 32ms/step - loss: 3.4581\n",
      "\n",
      "Epoch 00002: loss improved from 3.51626 to 3.45811, saving model to weights-improvement-02-3.4581-bigger.hdf5\n",
      "Epoch 3/50\n",
      "13224/13224 [==============================] - 432s 33ms/step - loss: 3.4238\n",
      "\n",
      "Epoch 00003: loss improved from 3.45811 to 3.42379, saving model to weights-improvement-03-3.4238-bigger.hdf5\n",
      "Epoch 4/50\n",
      "13224/13224 [==============================] - 410s 31ms/step - loss: 3.3498\n",
      "\n",
      "Epoch 00004: loss improved from 3.42379 to 3.34984, saving model to weights-improvement-04-3.3498-bigger.hdf5\n",
      "Epoch 5/50\n",
      "13224/13224 [==============================] - 570s 43ms/step - loss: 3.2215\n",
      "\n",
      "Epoch 00005: loss improved from 3.34984 to 3.22149, saving model to weights-improvement-05-3.2215-bigger.hdf5\n",
      "Epoch 6/50\n",
      "13224/13224 [==============================] - 563s 43ms/step - loss: 3.0729\n",
      "\n",
      "Epoch 00006: loss improved from 3.22149 to 3.07287, saving model to weights-improvement-06-3.0729-bigger.hdf5\n",
      "Epoch 7/50\n",
      "13224/13224 [==============================] - 582s 44ms/step - loss: 2.9913\n",
      "\n",
      "Epoch 00007: loss improved from 3.07287 to 2.99125, saving model to weights-improvement-07-2.9913-bigger.hdf5\n",
      "Epoch 8/50\n",
      "13224/13224 [==============================] - 540s 41ms/step - loss: 2.9396\n",
      "\n",
      "Epoch 00008: loss improved from 2.99125 to 2.93961, saving model to weights-improvement-08-2.9396-bigger.hdf5\n",
      "Epoch 9/50\n",
      "13224/13224 [==============================] - 543s 41ms/step - loss: 2.8951\n",
      "\n",
      "Epoch 00009: loss improved from 2.93961 to 2.89513, saving model to weights-improvement-09-2.8951-bigger.hdf5\n",
      "Epoch 10/50\n",
      "13224/13224 [==============================] - 472s 36ms/step - loss: 2.8485\n",
      "\n",
      "Epoch 00010: loss improved from 2.89513 to 2.84852, saving model to weights-improvement-10-2.8485-bigger.hdf5\n",
      "Epoch 11/50\n",
      "13224/13224 [==============================] - 443s 34ms/step - loss: 2.8042\n",
      "\n",
      "Epoch 00011: loss improved from 2.84852 to 2.80418, saving model to weights-improvement-11-2.8042-bigger.hdf5\n",
      "Epoch 12/50\n",
      "13224/13224 [==============================] - 411s 31ms/step - loss: 2.7470\n",
      "\n",
      "Epoch 00012: loss improved from 2.80418 to 2.74697, saving model to weights-improvement-12-2.7470-bigger.hdf5\n",
      "Epoch 13/50\n",
      "13224/13224 [==============================] - 435s 33ms/step - loss: 2.7027\n",
      "\n",
      "Epoch 00013: loss improved from 2.74697 to 2.70275, saving model to weights-improvement-13-2.7027-bigger.hdf5\n",
      "Epoch 14/50\n",
      "13224/13224 [==============================] - 449s 34ms/step - loss: 2.6470\n",
      "\n",
      "Epoch 00014: loss improved from 2.70275 to 2.64699, saving model to weights-improvement-14-2.6470-bigger.hdf5\n",
      "Epoch 15/50\n",
      "13224/13224 [==============================] - 446s 34ms/step - loss: 2.5925\n",
      "\n",
      "Epoch 00015: loss improved from 2.64699 to 2.59247, saving model to weights-improvement-15-2.5925-bigger.hdf5\n",
      "Epoch 16/50\n",
      "13224/13224 [==============================] - 457s 35ms/step - loss: 2.5329\n",
      "\n",
      "Epoch 00016: loss improved from 2.59247 to 2.53293, saving model to weights-improvement-16-2.5329-bigger.hdf5\n",
      "Epoch 17/50\n",
      "13224/13224 [==============================] - 359s 27ms/step - loss: 2.4697\n",
      "\n",
      "Epoch 00017: loss improved from 2.53293 to 2.46975, saving model to weights-improvement-17-2.4697-bigger.hdf5\n",
      "Epoch 18/50\n",
      "13224/13224 [==============================] - 374s 28ms/step - loss: 2.4125\n",
      "\n",
      "Epoch 00018: loss improved from 2.46975 to 2.41248, saving model to weights-improvement-18-2.4125-bigger.hdf5\n",
      "Epoch 19/50\n",
      "13224/13224 [==============================] - 381s 29ms/step - loss: 2.3511\n",
      "\n",
      "Epoch 00019: loss improved from 2.41248 to 2.35113, saving model to weights-improvement-19-2.3511-bigger.hdf5\n",
      "Epoch 20/50\n",
      "13224/13224 [==============================] - 382s 29ms/step - loss: 2.2778\n",
      "\n",
      "Epoch 00020: loss improved from 2.35113 to 2.27776, saving model to weights-improvement-20-2.2778-bigger.hdf5\n",
      "Epoch 21/50\n",
      "13224/13224 [==============================] - 387s 29ms/step - loss: 2.2161\n",
      "\n",
      "Epoch 00021: loss improved from 2.27776 to 2.21605, saving model to weights-improvement-21-2.2161-bigger.hdf5\n",
      "Epoch 22/50\n",
      "13224/13224 [==============================] - 385s 29ms/step - loss: 2.1490\n",
      "\n",
      "Epoch 00022: loss improved from 2.21605 to 2.14896, saving model to weights-improvement-22-2.1490-bigger.hdf5\n",
      "Epoch 23/50\n",
      "13224/13224 [==============================] - 388s 29ms/step - loss: 2.0829\n",
      "\n",
      "Epoch 00023: loss improved from 2.14896 to 2.08293, saving model to weights-improvement-23-2.0829-bigger.hdf5\n",
      "Epoch 24/50\n",
      "13224/13224 [==============================] - 387s 29ms/step - loss: 2.0029\n",
      "\n",
      "Epoch 00024: loss improved from 2.08293 to 2.00293, saving model to weights-improvement-24-2.0029-bigger.hdf5\n",
      "Epoch 25/50\n",
      "13224/13224 [==============================] - 385s 29ms/step - loss: 1.9405\n",
      "\n",
      "Epoch 00025: loss improved from 2.00293 to 1.94046, saving model to weights-improvement-25-1.9405-bigger.hdf5\n",
      "Epoch 26/50\n",
      "13224/13224 [==============================] - 388s 29ms/step - loss: 1.8745\n",
      "\n",
      "Epoch 00026: loss improved from 1.94046 to 1.87448, saving model to weights-improvement-26-1.8745-bigger.hdf5\n",
      "Epoch 27/50\n",
      "13224/13224 [==============================] - 389s 29ms/step - loss: 1.8004\n",
      "\n",
      "Epoch 00027: loss improved from 1.87448 to 1.80044, saving model to weights-improvement-27-1.8004-bigger.hdf5\n",
      "Epoch 28/50\n",
      "13224/13224 [==============================] - 387s 29ms/step - loss: 1.7241\n",
      "\n",
      "Epoch 00028: loss improved from 1.80044 to 1.72410, saving model to weights-improvement-28-1.7241-bigger.hdf5\n",
      "Epoch 29/50\n",
      "13224/13224 [==============================] - 387s 29ms/step - loss: 1.6676\n",
      "\n",
      "Epoch 00029: loss improved from 1.72410 to 1.66758, saving model to weights-improvement-29-1.6676-bigger.hdf5\n",
      "Epoch 30/50\n",
      "13224/13224 [==============================] - 391s 30ms/step - loss: 1.5958\n",
      "\n",
      "Epoch 00030: loss improved from 1.66758 to 1.59578, saving model to weights-improvement-30-1.5958-bigger.hdf5\n",
      "Epoch 31/50\n",
      "13224/13224 [==============================] - 384s 29ms/step - loss: 1.5294\n",
      "\n",
      "Epoch 00031: loss improved from 1.59578 to 1.52938, saving model to weights-improvement-31-1.5294-bigger.hdf5\n",
      "Epoch 32/50\n",
      "13224/13224 [==============================] - 394s 30ms/step - loss: 1.4642\n",
      "\n",
      "Epoch 00032: loss improved from 1.52938 to 1.46418, saving model to weights-improvement-32-1.4642-bigger.hdf5\n",
      "Epoch 33/50\n",
      "13224/13224 [==============================] - 387s 29ms/step - loss: 1.4035\n",
      "\n",
      "Epoch 00033: loss improved from 1.46418 to 1.40345, saving model to weights-improvement-33-1.4035-bigger.hdf5\n",
      "Epoch 34/50\n",
      "13224/13224 [==============================] - 390s 30ms/step - loss: 1.3383\n",
      "\n",
      "Epoch 00034: loss improved from 1.40345 to 1.33830, saving model to weights-improvement-34-1.3383-bigger.hdf5\n",
      "Epoch 35/50\n",
      "13224/13224 [==============================] - 392s 30ms/step - loss: 1.2822\n",
      "\n",
      "Epoch 00035: loss improved from 1.33830 to 1.28216, saving model to weights-improvement-35-1.2822-bigger.hdf5\n",
      "Epoch 36/50\n",
      "13224/13224 [==============================] - 386s 29ms/step - loss: 1.2344\n",
      "\n",
      "Epoch 00036: loss improved from 1.28216 to 1.23440, saving model to weights-improvement-36-1.2344-bigger.hdf5\n",
      "Epoch 37/50\n",
      "13224/13224 [==============================] - 391s 30ms/step - loss: 1.1657\n",
      "\n",
      "Epoch 00037: loss improved from 1.23440 to 1.16569, saving model to weights-improvement-37-1.1657-bigger.hdf5\n",
      "Epoch 38/50\n",
      "13224/13224 [==============================] - 385s 29ms/step - loss: 1.1286\n",
      "\n",
      "Epoch 00038: loss improved from 1.16569 to 1.12857, saving model to weights-improvement-38-1.1286-bigger.hdf5\n",
      "Epoch 39/50\n",
      "13224/13224 [==============================] - 385s 29ms/step - loss: 1.0668\n",
      "\n",
      "Epoch 00039: loss improved from 1.12857 to 1.06678, saving model to weights-improvement-39-1.0668-bigger.hdf5\n",
      "Epoch 40/50\n",
      "13224/13224 [==============================] - 390s 29ms/step - loss: 1.0260\n",
      "\n",
      "Epoch 00040: loss improved from 1.06678 to 1.02602, saving model to weights-improvement-40-1.0260-bigger.hdf5\n",
      "Epoch 41/50\n",
      "13224/13224 [==============================] - 388s 29ms/step - loss: 0.9731\n",
      "\n",
      "Epoch 00041: loss improved from 1.02602 to 0.97310, saving model to weights-improvement-41-0.9731-bigger.hdf5\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13224/13224 [==============================] - 389s 29ms/step - loss: 0.9385\n",
      "\n",
      "Epoch 00042: loss improved from 0.97310 to 0.93846, saving model to weights-improvement-42-0.9385-bigger.hdf5\n",
      "Epoch 43/50\n",
      "13224/13224 [==============================] - 391s 30ms/step - loss: 0.8801\n",
      "\n",
      "Epoch 00043: loss improved from 0.93846 to 0.88010, saving model to weights-improvement-43-0.8801-bigger.hdf5\n",
      "Epoch 44/50\n",
      "13224/13224 [==============================] - 387s 29ms/step - loss: 0.8378\n",
      "\n",
      "Epoch 00044: loss improved from 0.88010 to 0.83776, saving model to weights-improvement-44-0.8378-bigger.hdf5\n",
      "Epoch 45/50\n",
      "13224/13224 [==============================] - 384s 29ms/step - loss: 0.8178\n",
      "\n",
      "Epoch 00045: loss improved from 0.83776 to 0.81778, saving model to weights-improvement-45-0.8178-bigger.hdf5\n",
      "Epoch 46/50\n",
      "13224/13224 [==============================] - 390s 30ms/step - loss: 0.7835\n",
      "\n",
      "Epoch 00046: loss improved from 0.81778 to 0.78352, saving model to weights-improvement-46-0.7835-bigger.hdf5\n",
      "Epoch 47/50\n",
      "13224/13224 [==============================] - 387s 29ms/step - loss: 0.7468\n",
      "\n",
      "Epoch 00047: loss improved from 0.78352 to 0.74676, saving model to weights-improvement-47-0.7468-bigger.hdf5\n",
      "Epoch 48/50\n",
      "13224/13224 [==============================] - 388s 29ms/step - loss: 0.7094\n",
      "\n",
      "Epoch 00048: loss improved from 0.74676 to 0.70937, saving model to weights-improvement-48-0.7094-bigger.hdf5\n",
      "Epoch 49/50\n",
      "13224/13224 [==============================] - 385s 29ms/step - loss: 0.6815\n",
      "\n",
      "Epoch 00049: loss improved from 0.70937 to 0.68146, saving model to weights-improvement-49-0.6815-bigger.hdf5\n",
      "Epoch 50/50\n",
      "13224/13224 [==============================] - 386s 29ms/step - loss: 0.6409\n",
      "\n",
      "Epoch 00050: loss improved from 0.68146 to 0.64094, saving model to weights-improvement-50-0.6409-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3497165550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"2-layer-200-weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"2-layer-200-weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "ी मागेको पनि खुलासा गरिन्। फिल्म गुलामको अभिनेत्रीमा छानिएकी रानीको आवाज मोटो भएको र त्यो फिल्ममा नसुहाउने भन्दै प्रमुख अभिनेता आमिर खान, निर्देशक विक्रम भट्ट र निर्माता मुकेश भट्टले उनको भुमिकाका लाग \n",
      "\n",
      "ी मागेको पनि खुलासा गरिन्। फिल्म गुलामको अभिनेत्रीमा छानिएकी रानीको आवाज मोटो भएको र त्यो फिल्ममा नसुहाउने भन्दै प्रमुख अभिनेता आमिर खान, निर्देशक विक्रम भट्ट र निर्माता मुकेश भट्टले उनको भुमिकाका लागि आवाज डबिङ्ग हुने भएको छुनासो पनि सुन्नुपर्याक चन्दले आफूले विगतमा प्रुष्यिपक सहेकरले जाणीलाई प्राण गरेका थिए । फाटक घरको उदघाट च्रदाशन गर्ने सम्झिला माम्दै  दाज  थार्मेत  भ्रानिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन  प्रिनिन \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (''.join([int_to_char[value] for value in pattern]), \"\\n\")\n",
    "print(\"\".join([int_to_char[value] for value in pattern]),end=\"\")\n",
    "# generate characters\n",
    "f = open(\"sent_dump\",\"w\")\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    print(result,sep='',end='')\n",
    "    f.write(\"\".join([int_to_char[int(j*n_vocab)] \n",
    "            for i in x[0]\n",
    "               for j in i\n",
    "           ])+\"-->\"+result+\"\\n\")\n",
    "    pattern.append(index)\n",
    "    \n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
