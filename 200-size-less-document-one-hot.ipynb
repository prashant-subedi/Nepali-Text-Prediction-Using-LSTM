{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "import sys\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os\n",
    "import sys\n",
    "wordlists = PlaintextCorpusReader(\"Nepali_Corpus\", '.*txt')\n",
    "data = wordlists.fileids()[:10]\n",
    "text = []\n",
    "invalid = 0\n",
    "for i in data:\n",
    "    with open(os.path.join(\"Nepali_Corpus\",i),encoding='utf8') as file:\n",
    "        try:\n",
    "            text.append(file.read())\n",
    "        except UnicodeDecodeError :\n",
    "            print(i)\n",
    "            invalid+=1\n",
    "print(invalid)\n",
    "raw_text = \"\".join(text)\n",
    "raw_text = raw_text.replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '(', ')', ',', '.', '?', '\\xa0', '¥', 'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ए', 'ऐ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'े', 'ै', 'ो', 'ौ', '्', '।', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '–', '‘', '’']\n",
      "Total Characters:  13424\n",
      "Total Vocab:  78\n",
      "Total Patterns:  13224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 200\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = np_utils.to_categorical(X)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13224, 200, 78)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13224/13224 [==============================] - 191s 14ms/step - loss: 3.4924\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.49244, saving model to weights-improvement-01-3.4924-bigger.hdf5\n",
      "Epoch 2/50\n",
      "13224/13224 [==============================] - 187s 14ms/step - loss: 3.1520\n",
      "\n",
      "Epoch 00002: loss improved from 3.49244 to 3.15199, saving model to weights-improvement-02-3.1520-bigger.hdf5\n",
      "Epoch 3/50\n",
      "13224/13224 [==============================] - 187s 14ms/step - loss: 2.8391\n",
      "\n",
      "Epoch 00003: loss improved from 3.15199 to 2.83912, saving model to weights-improvement-03-2.8391-bigger.hdf5\n",
      "Epoch 4/50\n",
      "13224/13224 [==============================] - 187s 14ms/step - loss: 2.6856\n",
      "\n",
      "Epoch 00004: loss improved from 2.83912 to 2.68561, saving model to weights-improvement-04-2.6856-bigger.hdf5\n",
      "Epoch 5/50\n",
      "13224/13224 [==============================] - 188s 14ms/step - loss: 2.5354\n",
      "\n",
      "Epoch 00005: loss improved from 2.68561 to 2.53541, saving model to weights-improvement-05-2.5354-bigger.hdf5\n",
      "Epoch 6/50\n",
      "13224/13224 [==============================] - 187s 14ms/step - loss: 2.4064\n",
      "\n",
      "Epoch 00006: loss improved from 2.53541 to 2.40645, saving model to weights-improvement-06-2.4064-bigger.hdf5\n",
      "Epoch 7/50\n",
      "13224/13224 [==============================] - 187s 14ms/step - loss: 2.2861\n",
      "\n",
      "Epoch 00007: loss improved from 2.40645 to 2.28613, saving model to weights-improvement-07-2.2861-bigger.hdf5\n",
      "Epoch 8/50\n",
      "13224/13224 [==============================] - 186s 14ms/step - loss: 2.1722\n",
      "\n",
      "Epoch 00008: loss improved from 2.28613 to 2.17216, saving model to weights-improvement-08-2.1722-bigger.hdf5\n",
      "Epoch 9/50\n",
      "13224/13224 [==============================] - 186s 14ms/step - loss: 2.0563\n",
      "\n",
      "Epoch 00009: loss improved from 2.17216 to 2.05635, saving model to weights-improvement-09-2.0563-bigger.hdf5\n",
      "Epoch 10/50\n",
      "13224/13224 [==============================] - 186s 14ms/step - loss: 1.9424\n",
      "\n",
      "Epoch 00010: loss improved from 2.05635 to 1.94240, saving model to weights-improvement-10-1.9424-bigger.hdf5\n",
      "Epoch 11/50\n",
      "13224/13224 [==============================] - 195s 15ms/step - loss: 1.8363\n",
      "\n",
      "Epoch 00011: loss improved from 1.94240 to 1.83625, saving model to weights-improvement-11-1.8363-bigger.hdf5\n",
      "Epoch 12/50\n",
      "13224/13224 [==============================] - 197s 15ms/step - loss: 1.7239\n",
      "\n",
      "Epoch 00012: loss improved from 1.83625 to 1.72394, saving model to weights-improvement-12-1.7239-bigger.hdf5\n",
      "Epoch 13/50\n",
      "13224/13224 [==============================] - 188s 14ms/step - loss: 1.6010\n",
      "\n",
      "Epoch 00013: loss improved from 1.72394 to 1.60102, saving model to weights-improvement-13-1.6010-bigger.hdf5\n",
      "Epoch 14/50\n",
      "13224/13224 [==============================] - 187s 14ms/step - loss: 1.4854\n",
      "\n",
      "Epoch 00014: loss improved from 1.60102 to 1.48545, saving model to weights-improvement-14-1.4854-bigger.hdf5\n",
      "Epoch 15/50\n",
      "13224/13224 [==============================] - 188s 14ms/step - loss: 1.3728\n",
      "\n",
      "Epoch 00015: loss improved from 1.48545 to 1.37280, saving model to weights-improvement-15-1.3728-bigger.hdf5\n",
      "Epoch 16/50\n",
      "13224/13224 [==============================] - 188s 14ms/step - loss: 1.2441\n",
      "\n",
      "Epoch 00016: loss improved from 1.37280 to 1.24407, saving model to weights-improvement-16-1.2441-bigger.hdf5\n",
      "Epoch 17/50\n",
      "13224/13224 [==============================] - 188s 14ms/step - loss: 1.1370\n",
      "\n",
      "Epoch 00017: loss improved from 1.24407 to 1.13705, saving model to weights-improvement-17-1.1370-bigger.hdf5\n",
      "Epoch 18/50\n",
      "13224/13224 [==============================] - 187s 14ms/step - loss: 1.0281\n",
      "\n",
      "Epoch 00018: loss improved from 1.13705 to 1.02811, saving model to weights-improvement-18-1.0281-bigger.hdf5\n",
      "Epoch 19/50\n",
      "13224/13224 [==============================] - 188s 14ms/step - loss: 0.9192\n",
      "\n",
      "Epoch 00019: loss improved from 1.02811 to 0.91918, saving model to weights-improvement-19-0.9192-bigger.hdf5\n",
      "Epoch 20/50\n",
      "13224/13224 [==============================] - 188s 14ms/step - loss: 0.8289\n",
      "\n",
      "Epoch 00020: loss improved from 0.91918 to 0.82890, saving model to weights-improvement-20-0.8289-bigger.hdf5\n",
      "Epoch 21/50\n",
      "13224/13224 [==============================] - 188s 14ms/step - loss: 0.7308\n",
      "\n",
      "Epoch 00021: loss improved from 0.82890 to 0.73080, saving model to weights-improvement-21-0.7308-bigger.hdf5\n",
      "Epoch 22/50\n",
      "13224/13224 [==============================] - 188s 14ms/step - loss: 0.6426\n",
      "\n",
      "Epoch 00022: loss improved from 0.73080 to 0.64265, saving model to weights-improvement-22-0.6426-bigger.hdf5\n",
      "Epoch 23/50\n",
      "13224/13224 [==============================] - 192s 15ms/step - loss: 0.5698\n",
      "\n",
      "Epoch 00023: loss improved from 0.64265 to 0.56984, saving model to weights-improvement-23-0.5698-bigger.hdf5\n",
      "Epoch 24/50\n",
      "13224/13224 [==============================] - 186s 14ms/step - loss: 0.5052\n",
      "\n",
      "Epoch 00024: loss improved from 0.56984 to 0.50524, saving model to weights-improvement-24-0.5052-bigger.hdf5\n",
      "Epoch 25/50\n",
      "13224/13224 [==============================] - 185s 14ms/step - loss: 0.4450\n",
      "\n",
      "Epoch 00025: loss improved from 0.50524 to 0.44504, saving model to weights-improvement-25-0.4450-bigger.hdf5\n",
      "Epoch 26/50\n",
      "13224/13224 [==============================] - 185s 14ms/step - loss: 0.4004\n",
      "\n",
      "Epoch 00026: loss improved from 0.44504 to 0.40035, saving model to weights-improvement-26-0.4004-bigger.hdf5\n",
      "Epoch 27/50\n",
      "13224/13224 [==============================] - 185s 14ms/step - loss: 0.3481\n",
      "\n",
      "Epoch 00027: loss improved from 0.40035 to 0.34808, saving model to weights-improvement-27-0.3481-bigger.hdf5\n",
      "Epoch 28/50\n",
      "13224/13224 [==============================] - 185s 14ms/step - loss: 0.3076\n",
      "\n",
      "Epoch 00028: loss improved from 0.34808 to 0.30764, saving model to weights-improvement-28-0.3076-bigger.hdf5\n",
      "Epoch 29/50\n",
      "13224/13224 [==============================] - 185s 14ms/step - loss: 0.2872\n",
      "\n",
      "Epoch 00029: loss improved from 0.30764 to 0.28716, saving model to weights-improvement-29-0.2872-bigger.hdf5\n",
      "Epoch 30/50\n",
      "13224/13224 [==============================] - 186s 14ms/step - loss: 0.2534\n",
      "\n",
      "Epoch 00030: loss improved from 0.28716 to 0.25341, saving model to weights-improvement-30-0.2534-bigger.hdf5\n",
      "Epoch 31/50\n",
      "13224/13224 [==============================] - 185s 14ms/step - loss: 0.2339\n",
      "\n",
      "Epoch 00031: loss improved from 0.25341 to 0.23387, saving model to weights-improvement-31-0.2339-bigger.hdf5\n",
      "Epoch 32/50\n",
      "13224/13224 [==============================] - 185s 14ms/step - loss: 0.2197\n",
      "\n",
      "Epoch 00032: loss improved from 0.23387 to 0.21966, saving model to weights-improvement-32-0.2197-bigger.hdf5\n",
      "Epoch 33/50\n",
      "13224/13224 [==============================] - 185s 14ms/step - loss: 0.2075\n",
      "\n",
      "Epoch 00033: loss improved from 0.21966 to 0.20753, saving model to weights-improvement-33-0.2075-bigger.hdf5\n",
      "Epoch 34/50\n",
      "13224/13224 [==============================] - 186s 14ms/step - loss: 0.1865\n",
      "\n",
      "Epoch 00034: loss improved from 0.20753 to 0.18649, saving model to weights-improvement-34-0.1865-bigger.hdf5\n",
      "Epoch 35/50\n",
      "13224/13224 [==============================] - 185s 14ms/step - loss: 0.1738\n",
      "\n",
      "Epoch 00035: loss improved from 0.18649 to 0.17383, saving model to weights-improvement-35-0.1738-bigger.hdf5\n",
      "Epoch 36/50\n",
      "13224/13224 [==============================] - 185s 14ms/step - loss: 0.1568\n",
      "\n",
      "Epoch 00036: loss improved from 0.17383 to 0.15678, saving model to weights-improvement-36-0.1568-bigger.hdf5\n",
      "Epoch 37/50\n",
      "13224/13224 [==============================] - 193s 15ms/step - loss: 0.1565\n",
      "\n",
      "Epoch 00037: loss improved from 0.15678 to 0.15651, saving model to weights-improvement-37-0.1565-bigger.hdf5\n",
      "Epoch 38/50\n",
      "13224/13224 [==============================] - 186s 14ms/step - loss: 0.1457\n",
      "\n",
      "Epoch 00038: loss improved from 0.15651 to 0.14567, saving model to weights-improvement-38-0.1457-bigger.hdf5\n",
      "Epoch 39/50\n",
      "13224/13224 [==============================] - 187s 14ms/step - loss: 0.1346\n",
      "\n",
      "Epoch 00039: loss improved from 0.14567 to 0.13459, saving model to weights-improvement-39-0.1346-bigger.hdf5\n",
      "Epoch 40/50\n",
      "13224/13224 [==============================] - 187s 14ms/step - loss: 0.1299\n",
      "\n",
      "Epoch 00040: loss improved from 0.13459 to 0.12989, saving model to weights-improvement-40-0.1299-bigger.hdf5\n",
      "Epoch 41/50\n",
      "13224/13224 [==============================] - 187s 14ms/step - loss: 0.1324\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.12989\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13224/13224 [==============================] - 186s 14ms/step - loss: 0.1284\n",
      "\n",
      "Epoch 00042: loss improved from 0.12989 to 0.12842, saving model to weights-improvement-42-0.1284-bigger.hdf5\n",
      "Epoch 43/50\n",
      "13224/13224 [==============================] - 186s 14ms/step - loss: 0.1150\n",
      "\n",
      "Epoch 00043: loss improved from 0.12842 to 0.11500, saving model to weights-improvement-43-0.1150-bigger.hdf5\n",
      "Epoch 44/50\n",
      "13224/13224 [==============================] - 187s 14ms/step - loss: 0.1244\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.11500\n",
      "Epoch 45/50\n",
      "13224/13224 [==============================] - 186s 14ms/step - loss: 0.1233\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.11500\n",
      "Epoch 46/50\n",
      "13224/13224 [==============================] - 186s 14ms/step - loss: 0.1197\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.11500\n",
      "Epoch 47/50\n",
      "13224/13224 [==============================] - 188s 14ms/step - loss: 0.1151\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.11500\n",
      "Epoch 48/50\n",
      "13224/13224 [==============================] - 186s 14ms/step - loss: 0.1030\n",
      "\n",
      "Epoch 00048: loss improved from 0.11500 to 0.10299, saving model to weights-improvement-48-0.1030-bigger.hdf5\n",
      "Epoch 49/50\n",
      "13224/13224 [==============================] - 208s 16ms/step - loss: 0.0886\n",
      "\n",
      "Epoch 00049: loss improved from 0.10299 to 0.08862, saving model to weights-improvement-49-0.0886-bigger.hdf5\n",
      "Epoch 50/50\n",
      "13224/13224 [==============================] - 207s 16ms/step - loss: 0.1034\n",
      "\n",
      "Epoch 00050: loss did not improve from 0.08862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf1a4b45c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, validation_split=0.2,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"2-layer-200-weights-one-hot.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"2-layer-200-weights-one-hot.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Seed:\n",
      "े त्यसबेला आफूले बाल्यावस्थामा कविताबाटै झगडा गर्ने गरेको स्मरण गरिन् । ‘बाबाका कृतिहरू यसरी एकमुष्ठ प्रकाशित भएपछि अब उहाँलाई पढ्न सजिलो भएको छ,’ डा. पाण्डेले भनिन् । कृतिका सम्पादक प्राध्यापक सनतकुम \n",
      "\n",
      "े त्यसबेला आफूले बाल्यावस्थामा कविताबाटै झगडा गर्ने गरेको स्मरण गरिन् । ‘बाबाका कृतिहरू यसरी एकमुष्ठ प्रकाशित भएपछि अब उहाँलाई पढ्न सजिलो भएको छ,’ डा. पाण्डेले भनिन् । कृतिका सम्पादक प्राध्यापक सनतकुमारले आफू लेखनाथको साहित्यको पुजारी रहेकाले उनको कृतिमा काम गर्न पाउनु मनले रुचाएको अनुष्ठान गर्नु रहेको जनाए । उनले नै लेखनाथको रचनाको खोजबिन गरेर कृतिलाई पूर्णता दिएका हुन् । पुस्तकलाई नेपालयले प्रकाशन गरेको हो।  कामा अभिनेता राईलाई आइतबार सद्भावना दूत बनाइएको संग्रहालय निर्माण समितिका संयोजक खुक्सङ खम्बुले बताए । काठमाडौं – गीतकार तथा गजलकार कल्पना श्रेष्ठ कल्पपुञ्जले ‘सञ्जीवनी’ उपन्यास लिएर पाठकमाझ आएकी छन । शुक्रबार राजधानीमा एक कार्यक्रमबीच त्रि.वि.का उप प्राध्यापक महेश पौडेल, प्रा. डा. रजनी ढकाल र समीक्षक तथा लेखक राम लोहनीलगायतले पुस्तकको बारेमा समीक्षात्मक टिप्पणी गरेका थिए । उनले मलाई विश्वास गरे र मलाई तिम्रो आवाज तिम्रो आत्मा हो भने। उनको यो विश्वासले म मा आत्मविश्वास जगायो र हिम्मत बढायो।उनका अनुसार करणले बलिउडकै सर्वाधिक हिट फिल्म ‘कुछ कुछ होता है’मा वास्तविक आवाज राख्ने निर्णय सुनाए। रानी भन्छिन– करण नयाँ निर्देशक थिए। उनी मेरो भुमिकाको आवाज कुनै अरुसँग डब गराउन सक्थें। तर उनले मलाई विश्वास गरे र मलाई तिम्रो आवाज तिम्रो आत्मा हो भने। उनको यो विश्वासले म मा आत्मविश्वास जगा\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = X[start]\n",
    "print(pattern)\n",
    "print (\"Seed:\")\n",
    "print (''.join([int_to_char[numpy.argmax(value)] for value in pattern]), \"\\n\")\n",
    "print(\"\".join([int_to_char[numpy.argmax(value)] for value in pattern]),end=\"\")\n",
    "# generate characters\n",
    "f = open(\"sent_dump\",\"w\")\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), -1))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    print(result,sep='',end='')\n",
    "#     f.write(\"\".join([int_to_char[numpy.argmax(j)] \n",
    "#             for i in x[0]\n",
    "#                for j in i\n",
    "#            ])+\"-->\"+result+\"\\n\")\n",
    "    next = np_utils.to_categorical(index,num_classes=n_vocab).reshape(1,-1)    \n",
    "    pattern = numpy.vstack((pattern,next))\n",
    "    pattern = pattern[1:,:]\n",
    "print (\"\\nDone.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
